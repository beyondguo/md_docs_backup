---
title: 如何理解Inductive bias（归纳偏置）
published: 2021-12-13
sidebar: auto
---

# 如何理解Inductive bias（归纳偏置）？

> 近日看论文，总是看到inductive bias（归纳偏置）这个词，虽然之前也听过，但其实也没去仔细了解，遂趁机去网上查了查相关资料，翻了翻知乎，学习到了很多。这里记录一下，分享给大家。



## 知乎作者 TniL：

归纳偏置在机器学习中是一种很微妙的概念：在机器学习中，很多学习算法经常会对学习的问题做一些假设，这些假设就称为归纳偏置(Inductive Bias)。归纳偏置这个译名可能不能很好地帮助理解，不妨拆解开来看：归纳(Induction)是自然科学中常用的两大方法之一(归纳与演绎, induction and deduction)，指的是从一些例子中寻找共性、泛化，形成一个比较通用的规则的过程；偏置(Bias)是指我们对模型的偏好。因此，归纳偏置可以理解为，从现实生活中观察到的现象中归纳出一定的规则(heuristics)，然后对模型做一定的约束，从而可以起到“模型选择”的作用，即从假设空间中选择出更符合现实规则的模型。其实，贝叶斯学习中的“先验(Prior)”这个叫法，可能比“归纳偏置”更直观一些。归纳偏置在机器学习中几乎无处不可见。老生常谈的“奥卡姆剃刀”原理，即希望学习到的模型复杂度更低，就是一种归纳偏置。另外，还可以看见一些更强的一些假设：KNN中假设特征空间中相邻的样本倾向于属于同一类；SVM中假设好的分类器应该最大化类别边界距离；等等。在深度学习方面也是一样。以神经网络为例，各式各样的网络结构/组件/机制往往就来源于归纳偏置。在卷积神经网络中，我们假设特征具有局部性(Locality)的特性，即当我们把相邻的一些特征放在一起，会更容易得到“解”；在循环神经网络中，我们假设每一时刻的计算依赖于历史计算结果；还有注意力机制，也是基于从人的直觉、生活经验归纳得到的规则。在自然语言处理领域赫赫有名的word2vec，以及一些基于共现窗口的词嵌入方法，都是基于分布式假设：A word’s meaning is given by the words that frequently appear close-by. 这当然也可以看作是一种归纳偏置；一些自然语言理解的模型中加入解析树，也可以类似地理解。都是为了选择“更好”的模型。

- 回答链接🔗 ：https://www.zhihu.com/question/264264203/answer/830077823

## 知乎作者 rA9aoM：

No-Free-Lunch 不存在免费午餐理论提出学习是不可能的，除非有先验知识。通常情况下，我们不知道具体上帝函数的情况，但我们猜测它属于一个比较小的假设类别之中，这种基于先验知识对目标模型的判断就是Inductive bias-归纳误差。归纳误差所做的事情，是将无限可能的目标函数约束在一个有限的假设类别之中，这样，模型的学习才成为可能。

如果给出更加宽松的模型假设类别，即使用更弱的Inductive bias，那么我们更有可能得到强力模型-接近目标函数f。损失由近似损失和估计损失组成，这样做虽然减少了近似损失，但会增大估计损失，模型将更加难以学习，更容易过拟合。

- 回答链接🔗 ：https://www.zhihu.com/question/264264203/answer/1085271521


## 论文：Relational inductive biases, deep learning, and graph networks
这个论文也是知乎网友推荐的。我过去翻了翻，好像是一篇神作（目前引用量1300+）：

![image-20211213230018752](https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211213230025.png)

文章比较长，也不是我熟悉的领域，所以我只是随便瞅了瞅，里面有一段话和一个表，对inductive bias做了很详细清楚的解释：

![image-20211213230839925](https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211213230839.png)

上面这大段文字，我简单翻译总结一下：

- 归纳偏置，让算法优先某种解决方案，这种偏好是独立于观测的数据的。
- 常见的归纳偏置，包括：贝叶斯算法中的先验分布、使用某些正则项来惩罚模型、设计某种特殊的网络结构等。
- 好的归纳偏置，会提升算法搜索解的效率（同时不会怎么降低性能），而不好的归纳偏置则会让算法陷入次优解，因为它对算法带来了太强的限制。
- 归纳偏置，一般是对样本的产生过程，或者最终解的空间的一些假设。例如我们设计各种模型结构/形式，就是对解的空间上的假设。

然后，文章给出了下面这张图，总结了一下主要的神经网络结构背后的inductive bias是啥：

![image-20211213231601858](https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20211213231601.png)

可以看到，全连接网络的inductive bias是最轻微的，它就是假设所有的单元都可能会有联系；卷积则是假设数据的特征具有局部性和平移不变性，循环神经网络则是假设数据具有序列相关性和时序不变性，而图神经网络则是假设节点的特征的聚合方式是一致的。总之，网络的结构本身就包含了设计者的假设和偏好，这就是归纳偏置。





