---
title:  HuggingfaceğŸ¤—NLPç¬”è®°3ï¼šPipelineç«¯åˆ°ç«¯çš„èƒŒåå‘ç”Ÿäº†ä»€ä¹ˆ
published: 2021-9-24
sidebar: auto
---

> **ã€ŒHuggingfaceğŸ¤—NLPç¬”è®°ç³»åˆ—-ç¬¬3é›†ã€**
> æœ€è¿‘è·Ÿç€Huggingfaceä¸Šçš„NLP tutorialèµ°äº†ä¸€éï¼ŒæƒŠå¹å±…ç„¶æœ‰å¦‚æ­¤å¥½çš„è®²è§£Transformersç³»åˆ—çš„NLPæ•™ç¨‹ï¼Œäºæ˜¯å†³å®šè®°å½•ä¸€ä¸‹å­¦ä¹ çš„è¿‡ç¨‹ï¼Œåˆ†äº«æˆ‘çš„ç¬”è®°ï¼Œå¯ä»¥ç®—æ˜¯å®˜æ–¹æ•™ç¨‹çš„ç²¾ç®€ç‰ˆã€‚ä½†æœ€æ¨èçš„ï¼Œè¿˜æ˜¯ç›´æ¥è·Ÿç€å®˜æ–¹æ•™ç¨‹æ¥ä¸€éï¼ŒçœŸæ˜¯ä¸€ç§äº«å—ã€‚

- å®˜æ–¹æ•™ç¨‹ç½‘å€ï¼šhttps://huggingface.co/course/chapter1
- æœ¬æœŸå†…å®¹å¯¹åº”ç½‘å€ï¼šhttps://huggingface.co/course/chapter2/2?fw=pt
- æœ¬ç³»åˆ—ç¬”è®°çš„**GitHub**ï¼š https://github.com/beyondguo/Learn_PyTorch/tree/master/HuggingfaceNLP

---

# Pipelineç«¯åˆ°ç«¯çš„èƒŒåå‘ç”Ÿäº†ä»€ä¹ˆ

Pipelineçš„èƒŒåï¼š
<img src='https://huggingface.co/course/static/chapter2/full_nlp_pipeline.png' width=1000>

## 1. Tokenizer

æˆ‘ä»¬ä½¿ç”¨çš„tokenizerå¿…é¡»è·Ÿå¯¹åº”çš„æ¨¡å‹åœ¨é¢„è®­ç»ƒæ—¶çš„tokenizerä¿æŒä¸€è‡´ï¼Œä¹Ÿå°±æ˜¯è¯è¡¨éœ€è¦ä¸€è‡´ã€‚\
Huggingfaceä¸­å¯ä»¥ç›´æ¥æŒ‡å®šæ¨¡å‹çš„checkpointçš„åå­—ï¼Œç„¶åè‡ªåŠ¨ä¸‹è½½å¯¹åº”çš„è¯è¡¨ã€‚\
å…·ä½“æ–¹å¼æ˜¯ï¼š
- ä½¿ç”¨`AutoTokenizer`çš„`from_pretrained`æ–¹æ³•

`tokenizer`è¿™ä¸ªå¯¹è±¡å¯ä»¥ç›´æ¥æ¥å—å‚æ•°å¹¶è¾“å‡ºç»“æœï¼Œå³å®ƒæ˜¯callableçš„ã€‚å…·ä½“å‚æ•°è§ï¼š\
https://huggingface.co/transformers/master/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase \
ä¸»è¦å‚æ•°åŒ…æ‹¬ï¼š
- textï¼Œå¯ä»¥æ˜¯å•æ¡çš„stringï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªstringçš„listï¼Œè¿˜å¯ä»¥æ˜¯listçš„list
- paddingï¼Œç”¨äºå¡«ç™½
- truncationï¼Œç”¨äºæˆªæ–­
- max_lengthï¼Œè®¾ç½®æœ€å¤§å¥é•¿
- return_tensorsï¼Œè®¾ç½®è¿”å›æ•°æ®ç±»å‹


```python
from transformers import AutoTokenizer

checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

å…ˆçœ‹çœ‹ç›´æ¥ä½¿ç”¨tokenizerçš„ç»“æœï¼š


```python
raw_inputs = ['Today is a good day! Woo~~~',
              'How about tomorrow?']
tokenizer(raw_inputs)
```

è¾“å‡ºï¼š


```shell
{'input_ids': [[101, 2651, 2003, 1037, 2204, 2154, 999, 15854, 1066, 1066, 1066, 102], [101, 2129, 2055, 4826, 1029, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}
```



å¯ä»¥åŠ ä¸Šä¸€ä¸ª `padding=Ture` å‚æ•°ï¼Œè®©å¾—åˆ°çš„åºåˆ—é•¿åº¦å¯¹é½ï¼š


```python
tokenizer(raw_inputs, padding=True)
```

è¾“å‡ºï¼š


```shell
{'input_ids': [[101, 2651, 2003, 1037, 2204, 2154, 999, 15854, 1066, 1066, 1066, 102], [101, 2129, 2055, 4826, 1029, 102, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]}
```



tokenizerè¿˜æœ‰`truncation`å’Œ`max_length`å±æ€§ï¼Œç”¨äºåœ¨max_lengthå¤„æˆªæ–­ï¼š


```python
tokenizer(raw_inputs, padding=True, truncation=True, max_length=7) 
```

è¾“å‡ºï¼š


```shell
{'input_ids': [[101, 2651, 2003, 1037, 2204, 2154, 102], [101, 2129, 2055, 4826, 1029, 102, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0]]}
```



`return_tensors`å±æ€§ä¹Ÿå¾ˆé‡è¦ï¼Œç”¨æ¥æŒ‡å®šè¿”å›çš„æ˜¯ä»€ä¹ˆç±»å‹çš„tensorsï¼Œ`pt`å°±æ˜¯pytorchï¼Œ`tf`å°±æ˜¯tensorflowï¼š


```python
tokenizer(raw_inputs, padding=True, truncation=True, return_tensors='pt')
```

è¾“å‡ºï¼š


```shell
{'input_ids': tensor([[  101,  2651,  2003,  1037,  2204,  2154,   999, 15854,  1066,  1066,
          1066,   102],
        [  101,  2129,  2055,  4826,  1029,   102,     0,     0,     0,     0,
             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}
```



## 2. Model
ä¹Ÿå¯ä»¥é€šè¿‡AutoModelæ¥ç›´æ¥ä»checkpointå¯¼å…¥æ¨¡å‹ã€‚

è¿™é‡Œå¯¼å…¥çš„æ¨¡å‹ï¼Œæ˜¯Transformerçš„åŸºç¡€æ¨¡å‹ï¼Œæ¥å—tokenizeä¹‹åçš„è¾“å…¥ï¼Œ**è¾“å‡ºhidden statesï¼Œå³æ–‡æœ¬çš„å‘é‡è¡¨ç¤º**ï¼Œæ˜¯ä¸€ç§ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚

è¿™ä¸ªå‘é‡è¡¨ç¤ºï¼Œä¼šæœ‰ä¸‰ä¸ªç»´åº¦ï¼š
1. batch size
2. sequence length
3. hidden size


```python
from transformers import AutoModel
model = AutoModel.from_pretrained(checkpoint)
```

åŠ è½½äº†æ¨¡å‹ä¹‹åï¼Œå°±å¯ä»¥æŠŠtokenizerå¾—åˆ°çš„è¾“å‡ºï¼Œç›´æ¥è¾“å…¥åˆ°modelä¸­ï¼š


```python
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors='pt')
outputs = model(**inputs)  # è¿™é‡Œå˜é‡å‰é¢çš„**ï¼Œä»£è¡¨æŠŠinputsè¿™ä¸ªdictionaryç»™åˆ†è§£æˆä¸€ä¸ªä¸ªå‚æ•°å•ç‹¬è¾“è¿›å»
vars(outputs).keys()  # æŸ¥çœ‹ä¸€ä¸‹è¾“å‡ºæœ‰å“ªäº›å±æ€§
```

è¾“å‡ºï¼š


```shell
dict_keys(['last_hidden_state', 'hidden_states', 'attentions'])
```



>**è¿™é‡Œé¡ºä¾¿è®²ä¸€è®²è¿™ä¸ªå‡½æ•°ä¸­`**`çš„ç”¨æ³•ï¼š**

`**`åœ¨å‡½æ•°ä¸­çš„ä½œç”¨å°±æ˜¯æŠŠåé¢ç´§è·Ÿç€çš„è¿™ä¸ªå‚æ•°ï¼Œä»ä¸€ä¸ªå­—å…¸çš„æ ¼å¼ï¼Œè§£å‹æˆä¸€ä¸ªä¸ªå•ç‹¬çš„å‚æ•°ã€‚

å›é¡¾ä¸€ä¸‹ä¸Šé¢tokenizerçš„è¾“å‡ºï¼Œæˆ‘ä»¬å‘ç°å®ƒæ˜¯ä¸€ä¸ªåŒ…å«äº†input_idså’Œattention_maskä¸¤ä¸ªkeyçš„**å­—å…¸**ï¼Œå› æ­¤é€šè¿‡`**`çš„è§£å‹ï¼Œç›¸å½“äºå˜æˆäº†`intput_ids=..., attention_mask=...`å–‚ç»™å‡½æ•°ã€‚

æˆ‘ä»¬å†æ¥æŸ¥çœ‹ä¸€ä¸‹é€šè¿‡AutoModelåŠ è½½çš„DistillBertModelæ¨¡å‹çš„è¾“å…¥ï¼š
https://huggingface.co/transformers/master/model_doc/distilbert.html#distilbertmodel

å¯ä»¥çœ‹åˆ°DistillBertModelçš„ç›´æ¥callçš„å‡½æ•°æ˜¯ï¼š

`forward(input_ids=None, attention_mask=None, ...)`
æ­£å¥½è·Ÿ`**inputs`åçš„æ ¼å¼å¯¹åº”ä¸Šã€‚


```python
print(outputs.last_hidden_state.shape)
outputs.last_hidden_state
```
è¾“å‡º
```shell
torch.Size([2, 12, 768])

    tensor([[[ 0.4627,  0.3042,  0.5431,  ...,  0.3706,  1.0033, -0.6074],
             [ 0.6100,  0.3093,  0.2038,  ...,  0.3788,  0.9370, -0.6439],
             [ 0.6514,  0.3185,  0.3855,  ...,  0.4152,  1.0199, -0.4450],
             ...,
             [ 0.3674,  0.1380,  1.1619,  ...,  0.4976,  0.4758, -0.5896],
             [ 0.4182,  0.2503,  1.0898,  ...,  0.4745,  0.4042, -0.5444],
             [ 1.1614,  0.2516,  0.9561,  ...,  0.5742,  0.8437, -0.9604]],
    
            [[ 0.7956, -0.2343,  0.3810,  ..., -0.1270,  0.5182, -0.1612],
             [ 0.9337,  0.2074,  0.6202,  ...,  0.1874,  0.6584, -0.1899],
             [ 0.6279, -0.3176,  0.1596,  ..., -0.2956,  0.2960, -0.1447],
             ...,
             [ 0.3050,  0.0396,  0.6345,  ...,  0.4271,  0.3367, -0.3285],
             [ 0.1773,  0.0111,  0.6275,  ...,  0.3831,  0.3543, -0.2919],
             [ 0.2756,  0.0048,  0.9281,  ...,  0.2006,  0.4375, -0.3238]]],
           grad_fn=<NativeLayerNormBackward>)
```


å¯ä»¥çœ‹åˆ°ï¼Œè¾“å‡ºçš„shapeæ˜¯`torch.Size([2, 12, 768])`ï¼Œä¸‰ä¸ªç»´åº¦åˆ†åˆ«æ˜¯ batchï¼Œseq_lenå’Œhidden sizeã€‚




## 3. Model Heads
æ¨¡å‹å¤´ï¼Œæ¥åœ¨åŸºç¡€æ¨¡å‹çš„åé¢ï¼Œç”¨äºå°†hidden statesæ–‡æœ¬è¡¨ç¤ºè¿›ä¸€æ­¥å¤„ç†ï¼Œç”¨äºå…·ä½“çš„ä»»åŠ¡ã€‚

æ•´ä½“æ¡†æ¶å›¾ï¼š

<img src='https://huggingface.co/course/static/chapter2/transformer_and_head.png' width=1000>

Headä¸€èˆ¬æ˜¯ç”±è‹¥å¹²å±‚çš„çº¿æ€§å±‚æ¥æ„æˆçš„ã€‚

Transformersåº“ä¸­çš„ä¸»è¦æ¨¡å‹æ¶æ„æœ‰ï¼š
- *Model (retrieve the hidden states)
- *ForCausalLM
- *ForMaskedLM
- *ForMultipleChoice
- *ForQuestionAnswering
- *ForSequenceClassification
- *ForTokenClassification
- ...

å•çº¯çš„`*Model`ï¼Œå°±æ˜¯ä¸åŒ…å« Head çš„æ¨¡å‹ï¼Œè€Œæœ‰`For*`çš„åˆ™æ˜¯åŒ…å«äº†å…·ä½“ Head çš„æ¨¡å‹ã€‚

ä¾‹å¦‚ï¼Œå¯¹äºå‰é¢çš„é‚£ä¸ªåšåœ¨æƒ…æ„Ÿåˆ†æä¸Špretrainçš„checkpoint(distilbert-base-uncased-finetuned-sst-2-english)ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨åŒ…å« SequenceClassification çš„Headçš„æ¨¡å‹å»åŠ è½½ï¼Œå°±å¯ä»¥ç›´æ¥å¾—åˆ°å¯¹åº”åˆ†ç±»é—®é¢˜çš„logitsï¼Œè€Œä¸ä»…ä»…æ˜¯æ–‡æœ¬å‘é‡è¡¨ç¤ºã€‚


```python
from transformers import AutoModelForSequenceClassification
clf = AutoModelForSequenceClassification.from_pretrained(checkpoint)
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors='pt')
outputs = clf(**inputs)
print(vars(outputs).keys())
outputs.logits
```
è¾“å‡ºï¼š
```shell
dict_keys(['loss', 'logits', 'hidden_states', 'attentions'])

tensor([[-4.2098,  4.6444],
        [ 0.6367, -0.3753]], grad_fn=<AddmmBackward>)
```



ä»outputsçš„å±æ€§å°±å¯ä»¥çœ‹å‡ºï¼Œå¸¦æœ‰Headçš„Modelï¼Œè·Ÿä¸å¸¦Headçš„Modelï¼Œè¾“å‡ºçš„ä¸œè¥¿æ˜¯ä¸ä¸€æ ·çš„ã€‚

**æ²¡æœ‰Headçš„Model**ï¼Œè¾“å‡ºçš„æ˜¯`'last_hidden_state', 'hidden_states', 'attentions'`è¿™äº›ç©æ„å„¿ï¼Œå› ä¸ºå®ƒä»…ä»…æ˜¯ä¸€ä¸ªè¡¨ç¤ºæ¨¡å‹ï¼›

**æœ‰Headçš„Model**ï¼Œè¾“å‡ºçš„æ˜¯`'loss', 'logits', 'hidden_states', 'attentions'`è¿™äº›ç©æ„å„¿ï¼Œæœ‰logitsï¼Œlossè¿™äº›ä¸œè¥¿ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªå®Œæ•´çš„é¢„æµ‹æ¨¡å‹äº†ã€‚

å¯ä»¥é¡ºä¾¿çœ‹çœ‹ï¼ŒåŠ äº†è¿™ä¸ª SequenceClassification Headçš„DistillBertModelçš„æ–‡æ¡£ï¼Œçœ‹çœ‹å…¶è¾“å…¥å’Œè¾“å‡ºï¼š

https://huggingface.co/transformers/master/model_doc/distilbert.html#distilbertforsequenceclassification

å¯ä»¥çœ‹åˆ°ï¼Œè¾“å…¥ä¸­ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æä¾›`labels`ï¼Œè¿™æ ·å°±å¯ä»¥ç›´æ¥è®¡ç®—lossäº†ã€‚

## 4. Post-Processing
åå¤„ç†ä¸»è¦å°±æ˜¯ä¸¤æ­¥ï¼š
- æŠŠlogitsè½¬åŒ–æˆæ¦‚ç‡å€¼ ï¼ˆç”¨softmaxï¼‰
- æŠŠæ¦‚ç‡å€¼è·Ÿå…·ä½“çš„æ ‡ç­¾å¯¹åº”ä¸Š ï¼ˆä½¿ç”¨æ¨¡å‹çš„configä¸­çš„id2labelï¼‰


```python
import torch
predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)  # dim=-1å°±æ˜¯æ²¿ç€æœ€åä¸€ç»´è¿›è¡Œæ“ä½œ
predictions
```

è¾“å‡ºï¼š


```shell
tensor([[1.4276e-04, 9.9986e-01],
        [7.3341e-01, 2.6659e-01]], grad_fn=<SoftmaxBackward>)
```



å¾—åˆ°äº†æ¦‚ç‡åˆ†å¸ƒï¼Œè¿˜å¾—çŸ¥é“å…·ä½“æ˜¯å•¥æ ‡ç­¾å§ã€‚æ ‡ç­¾è·Ÿidçš„éšå°„å…³ç³»ï¼Œä¹Ÿå·²ç»è¢«ä¿å­˜åœ¨æ¯ä¸ªpretrain modelçš„configä¸­äº†ï¼Œ
æˆ‘ä»¬å¯ä»¥å»æ¨¡å‹çš„`config`å±æ€§ä¸­æŸ¥çœ‹`id2label`å­—æ®µï¼š


```python
id2label = clf.config.id2label
id2label
```

è¾“å‡ºï¼š


```shell
{0: 'NEGATIVE', 1: 'POSITIVE'}
```

ç»¼åˆèµ·æ¥ï¼Œç›´æ¥ä»predictionå¾—åˆ°æ ‡ç­¾ï¼š


```python
for i in torch.argmax(predictions, dim=-1):
    print(id2label[i.item()])
```

è¾“å‡ºï¼š
```shell
    POSITIVE
    NEGATIVE
```

