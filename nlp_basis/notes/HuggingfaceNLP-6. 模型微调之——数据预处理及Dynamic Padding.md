---
title:  HuggingfaceğŸ¤—NLPç¬”è®°6ï¼šæ•°æ®é›†é¢„å¤„ç†ï¼Œä½¿ç”¨dynamic paddingæ„é€ batch
published: 2021-9-27
sidebar: auto
---

> **ã€ŒHuggingfaceğŸ¤—NLPç¬”è®°ç³»åˆ—-ç¬¬6é›†ã€**
> æœ€è¿‘è·Ÿç€Huggingfaceä¸Šçš„NLP tutorialèµ°äº†ä¸€éï¼ŒæƒŠå¹å±…ç„¶æœ‰å¦‚æ­¤å¥½çš„è®²è§£Transformersç³»åˆ—çš„NLPæ•™ç¨‹ï¼Œäºæ˜¯å†³å®šè®°å½•ä¸€ä¸‹å­¦ä¹ çš„è¿‡ç¨‹ï¼Œåˆ†äº«æˆ‘çš„ç¬”è®°ï¼Œå¯ä»¥ç®—æ˜¯å®˜æ–¹æ•™ç¨‹çš„**ç²¾ç®€+æ³¨è§£ç‰ˆ**ã€‚ä½†æœ€æ¨èçš„ï¼Œè¿˜æ˜¯ç›´æ¥è·Ÿç€å®˜æ–¹æ•™ç¨‹æ¥ä¸€éï¼ŒçœŸæ˜¯ä¸€ç§äº«å—ã€‚

- å®˜æ–¹æ•™ç¨‹ç½‘å€ï¼šhttps://huggingface.co/course/chapter1
- æœ¬æœŸå†…å®¹å¯¹åº”ç½‘å€ï¼šhttps://huggingface.co/course/chapter3/2?fw=pt
- æœ¬ç³»åˆ—ç¬”è®°çš„**GitHub**ï¼š https://github.com/beyondguo/Learn_PyTorch/tree/master/HuggingfaceNLP

---

# æ•°æ®é›†çš„é¢„å¤„ç†ï¼Œä½¿ç”¨dynamic paddingæ„é€ batch

ä»è¿™ä¸€é›†ï¼Œæˆ‘ä»¬å°±æ­£å¼å¼€å§‹ä½¿ç”¨Transformeræ¥è®­ç»ƒæ¨¡å‹äº†ã€‚ä»Šå¤©çš„éƒ¨åˆ†æ˜¯å…³äºæ•°æ®é›†é¢„å¤„ç†ã€‚

## è¯•ç€è®­ç»ƒä¸€ä¸¤æ¡æ ·æœ¬


```python
# å…ˆçœ‹çœ‹cudaæ˜¯å¦å¯ç”¨
import torch
torch.cuda.is_available()
```


```shell
>>> True
```

é¦–å…ˆï¼Œæˆ‘ä»¬åŠ è½½æ¨¡å‹ã€‚æ—¢ç„¶æ¨¡å‹è¦åœ¨å…·ä½“ä»»åŠ¡ä¸Šå¾®è°ƒäº†ï¼Œæˆ‘ä»¬å°±è¦åŠ è½½å¸¦æœ‰Headçš„æ¨¡å‹ï¼Œè¿™é‡Œåšçš„åˆ†ç±»é—®é¢˜ï¼Œå› æ­¤åŠ è½½`ForSequenceClassification`è¿™ä¸ªHeadï¼š


```python
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
```
ä¸‹é¢æ˜¯æ¨¡å‹è¾“å‡ºçš„warningï¼š
```shell
>>> 
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

çœ‹åˆ°è¿™ä¹ˆä¸€å¤§ä¸²çš„warningå‡ºç°ï¼Œä¸è¦æ€•ï¼Œè¿™ä¸ªwarningæ­£æ˜¯æˆ‘ä»¬å¸Œæœ›çœ‹åˆ°çš„ã€‚

ä¸ºå•¥ä¼šå‡ºç°è¿™ä¸ªwarningå‘¢ï¼Œå› ä¸ºæˆ‘ä»¬åŠ è½½çš„é¢„è®­ç»ƒæƒé‡æ˜¯`bert-based-uncased`ï¼Œè€Œä½¿ç”¨çš„éª¨æ¶æ˜¯`AutoModelForSequenceClassification`ï¼Œå‰è€…æ˜¯æ²¡æœ‰åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¾®è°ƒè¿‡çš„ï¼Œæ‰€ä»¥ç”¨å¸¦æœ‰ä¸‹æ¸¸ä»»åŠ¡Headçš„éª¨æ¶å»åŠ è½½ï¼Œä¼šéšæœºåˆå§‹åŒ–è¿™ä¸ªHeadã€‚è¿™äº›åœ¨warningä¸­ä¹Ÿè¯´çš„å¾ˆæ˜ç™½ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¯•è¯•ç›´æ¥æ„é€ ä¸€ä¸ªsize=2çš„batchï¼Œä¸¢è¿›æ¨¡å‹å»ã€‚

å½“è¾“å…¥çš„batchæ˜¯å¸¦æœ‰"labels"å±æ€§çš„æ—¶å€™ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨è®¡ç®—lossï¼Œæ‹¿ç€è¿™ä¸ªlossï¼Œæˆ‘ä»¬å°±å¯ä»¥è¿›è¡Œåå‘ä¼ æ’­å¹¶æ›´æ–°å‚æ•°äº†ï¼š

```python
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
batch['labels'] = torch.tensor([1, 1])  # tokenizerå‡ºæ¥çš„ç»“æœæ˜¯ä¸€ä¸ªdictionaryï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥åŠ å…¥æ–°çš„ key-value

optimizer = AdamW(model.parameters())
loss = model(**batch).loss  #è¿™é‡Œçš„ loss æ˜¯ç›´æ¥æ ¹æ® batch ä¸­æä¾›çš„ labels æ¥è®¡ç®—çš„ï¼Œå›å¿†ï¼šå‰é¢ç« èŠ‚æŸ¥çœ‹ model çš„è¾“å‡ºçš„æ—¶å€™ï¼Œæœ‰lossè¿™ä¸€é¡¹
loss.backward()
optimizer.step()
```

## ä»Huggingface Hubä¸­åŠ è½½æ•°æ®é›†

è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨MRPCæ•°æ®é›†ï¼Œå®ƒçš„å…¨ç§°æ˜¯Microsoft Research Paraphrase Corpusï¼ŒåŒ…å«äº†5801ä¸ªå¥å­å¯¹ï¼Œæ ‡ç­¾æ˜¯ä¸¤ä¸ªå¥å­æ˜¯å¦æ˜¯åŒä¸€ä¸ªæ„æ€ã€‚

Huggingfaceæœ‰ä¸€ä¸ª`datasets`åº“ï¼Œå¯ä»¥è®©æˆ‘ä»¬è½»æ¾åœ°ä¸‹è½½å¸¸è§çš„æ•°æ®é›†ï¼š


```python
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

çœ‹çœ‹åŠ è½½çš„datasetçš„æ ·å­ï¼š

```shell
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```



load_datasetå‡ºæ¥çš„æ˜¯ä¸€ä¸ªDatasetDictå¯¹è±¡ï¼Œå®ƒåŒ…å«äº†trainï¼Œvalidationï¼Œtestä¸‰ä¸ªå±æ€§ã€‚å¯ä»¥é€šè¿‡keyæ¥ç›´æ¥æŸ¥è¯¢ï¼Œå¾—åˆ°å¯¹åº”çš„trainã€validå’Œtestæ•°æ®é›†ã€‚

è¿™é‡Œçš„trainï¼Œvalidï¼Œtestéƒ½æ˜¯Datasetç±»å‹ï¼Œæœ‰ featureså’Œnum_rowsä¸¤ä¸ªå±æ€§ã€‚è¿˜å¯ä»¥ç›´æ¥é€šè¿‡ä¸‹æ ‡æ¥æŸ¥è¯¢å¯¹åº”çš„æ ·æœ¬ã€‚


```python
raw_train_dataset = raw_datasets['train']
raw_train_dataset[0]
```

çœ‹çœ‹æ•°æ®é•¿å•¥æ ·ï¼š


```shell
{'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .',
 'label': 1,
 'idx': 0}
```

å¯è§ï¼Œæ¯ä¸€æ¡æ•°æ®ï¼Œå°±æ˜¯ä¸€ä¸ªdictionaryã€‚

Datasetçš„featureså¯ä»¥ç†è§£ä¸ºä¸€å¼ è¡¨çš„columnsï¼ŒDatasetç”šè‡³å¯ä»¥çœ‹åšä¸€ä¸ªpandasçš„dataframeï¼ŒäºŒè€…çš„ä½¿ç”¨å¾ˆç±»ä¼¼ã€‚

æˆ‘ä»¬å¯ä»¥ç›´æ¥åƒæ“ä½œdataframeä¸€æ ·ï¼Œå–å‡ºæŸä¸€åˆ—ï¼š


```python
type(raw_train_dataset['sentence1'])  # ç›´æ¥å–å‡ºæ‰€æœ‰çš„sentence1ï¼Œå½¢æˆä¸€ä¸ªlist
```


```shellÂ 
>>> list
```



é€šè¿‡Datasetçš„featureså±æ€§ï¼Œå¯ä»¥è¯¦ç»†æŸ¥çœ‹æ•°æ®é›†ç‰¹å¾ï¼ŒåŒ…æ‹¬labelså…·ä½“éƒ½æ˜¯å•¥ï¼š


```python
raw_train_dataset.features
```


```shell
>>>
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```



## æ•°æ®é›†çš„é¢„å¤„ç†


```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
```

æˆ‘ä»¬å¯ä»¥ç›´æ¥ä¸‹é¢è¿™æ ·å¤„ç†ï¼š
```python
tokenized_sentences_1 = tokenizer(raw_train_dataset['sentence1'])
tokenized_sentences_2 = tokenizer(raw_train_dataset['sentence2'])
```
ä½†å¯¹äºMRPCä»»åŠ¡ï¼Œæˆ‘ä»¬ä¸èƒ½æŠŠä¸¤ä¸ªå¥å­åˆ†å¼€è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼ŒäºŒè€…åº”è¯¥ç»„æˆä¸€ä¸ªpairè¾“è¿›å»ã€‚

tokenizerä¹Ÿå¯ä»¥ç›´æ¥å¤„ç†sequence pairï¼š


```python
from pprint import pprint as print
inputs = tokenizer("first sentence", "second one")
print(inputs)
```

```shell
>>>
{'attention_mask': [1, 1, 1, 1, 1, 1, 1],
 'input_ids': [101, 2034, 6251, 102, 2117, 2028, 102],
 'token_type_ids': [0, 0, 0, 0, 1, 1, 1]}
```

æˆ‘ä»¬æŠŠè¿™é‡Œçš„input_idsç»™decodeçœ‹ä¸€ä¸‹ï¼š

```python
tokenizer.decode(inputs.input_ids)
```


```shell
>>>
'[CLS] first sentence [SEP] second one [SEP]'
```

å¯ä»¥çœ‹åˆ°è¿™é‡Œinputsé‡Œï¼Œè¿˜æœ‰ä¸€ä¸ª`token_type_ids`å±æ€§ï¼Œå®ƒåœ¨è¿™é‡Œçš„ä½œç”¨å°±å¾ˆæ˜æ˜¾äº†ï¼ŒæŒ‡ç¤ºå“ªäº›è¯æ˜¯å±äºç¬¬ä¸€ä¸ªå¥å­ï¼Œå“ªäº›è¯æ˜¯å±äºç¬¬äºŒä¸ªå¥å­ã€‚tokenizerå¤„ç†åå¾—åˆ°çš„idsï¼Œè§£ç ä¹‹åï¼Œåœ¨å¼€å¤´ç»“å°¾å¤šäº†`[CLS]`å’Œ`[SEP]`ï¼Œä¸¤ä¸ªå¥å­ä¸­é—´ä¹Ÿæ·»åŠ äº†ä¸€ä¸ª`[SEP]`ã€‚å¦å¤–æ³¨æ„ï¼Œè™½ç„¶è¾“å…¥çš„æ˜¯ä¸€ä¸ªå¥å­å¯¹ï¼Œä½†æ˜¯ç¼–ç ä¹‹åæ˜¯ä¸€ä¸ªæ•´ä½“ï¼Œé€šè¿‡`[SEP]`ç¬¦å·ç›¸è¿ã€‚

**è¿™ç§ç¥å¥‡çš„åšæ³•ï¼Œå…¶å®æ˜¯æºäºbert-baseé¢„è®­ç»ƒçš„ä»»åŠ¡**ï¼Œå³**next sentence prediction**ã€‚æ¢æˆå…¶ä»–æ¨¡å‹ï¼Œæ¯”å¦‚DistilBertï¼Œå®ƒåœ¨é¢„è®­ç»ƒçš„æ—¶å€™æ²¡æœ‰è¿™ä¸ªä»»åŠ¡ï¼Œé‚£å®ƒçš„tokenizerçš„ç»“æœå°±ä¸ä¼šæœ‰è¿™ä¸ª`token_type_ids`å±æ€§äº†ã€‚

æ—¢ç„¶è¿™é‡Œçš„tokenizerå¯ä»¥ç›´æ¥å¤„ç†pairï¼Œæˆ‘ä»¬å°±å¯ä»¥è¿™ä¹ˆå»åˆ†è¯ï¼š


```python
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

ä½†æ˜¯è¿™æ ·ä¸ä¸€å®šå¥½ï¼Œå› ä¸ºå…ˆæ˜¯ç›´æ¥æŠŠè¦å¤„ç†çš„æ•´ä¸ªæ•°æ®é›†éƒ½è¯»è¿›äº†å†…å­˜ï¼Œåˆè¿”å›ä¸€ä¸ªæ–°çš„dictionaryï¼Œä¼šå æ®å¾ˆå¤šå†…å­˜ã€‚

å®˜æ–¹æ¨èçš„åšæ³•æ˜¯é€šè¿‡`Dataset.map`æ–¹æ³•ï¼Œæ¥è°ƒç”¨ä¸€ä¸ªåˆ†è¯æ–¹æ³•ï¼Œå®ç°æ‰¹é‡åŒ–çš„åˆ†è¯ï¼š


```python
def tokenize_function(sample):
    # è¿™é‡Œå¯ä»¥æ·»åŠ å¤šç§æ“ä½œï¼Œä¸å…‰æ˜¯tokenize
    # è¿™ä¸ªå‡½æ•°å¤„ç†çš„å¯¹è±¡ï¼Œå°±æ˜¯Datasetè¿™ç§æ•°æ®ç±»å‹ï¼Œé€šè¿‡featuresä¸­çš„å­—æ®µæ¥é€‰æ‹©è¦å¤„ç†çš„æ•°æ®
    return tokenizer(sample['sentence1'], sample['sentence2'], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

å¤„ç†åçš„datasetçš„ä¿¡æ¯ï¼š

```shell
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```



çœ‹çœ‹è¿™ä¸ªmapçš„ä¸€äº›å‚æ•°ï¼š

```shell
raw_datasets.map(
    function,
    with_indices: bool = False,
    input_columns: Union[str, List[str], NoneType] = None,
    batched: bool = False,
    batch_size: Union[int, NoneType] = 1000,
    remove_columns: Union[str, List[str], NoneType] = None,
    keep_in_memory: bool = False,
    load_from_cache_file: bool = True,
    cache_file_names: Union[Dict[str, Union[str, NoneType]], NoneType] = None,
    writer_batch_size: Union[int, NoneType] = 1000,
    features: Union[datasets.features.Features, NoneType] = None,
    disable_nullable: bool = False,
    fn_kwargs: Union[dict, NoneType] = None,
    num_proc: Union[int, NoneType] = None,  # ä½¿ç”¨æ­¤å‚æ•°ï¼Œå¯ä»¥ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†
    desc: Union[str, NoneType] = None,
) -> 'DatasetDict'
Docstring:
Apply a function to all the elements in the table (individually or in batches)
and update the table (if function does updated examples).
The transformation is applied to all the datasets of the dataset dictionary.
```

å…³äºè¿™ä¸ªmapï¼Œåœ¨Huggingfaceçš„æµ‹è¯•é¢˜ä¸­æœ‰è®²è§£ï¼Œè¿™é‡Œæ¬è¿å¹¶ç¿»è¯‘ä¸€ä¸‹ï¼Œè¾…åŠ©ç†è§£ï¼š

### Dataset.mapæ–¹æ³•æœ‰å•¥å¥½å¤„ï¼š

- The results of the function are cached, so it won't take any time if we re-execute the code.

    ï¼ˆé€šè¿‡è¿™ä¸ªmapï¼Œå¯¹æ•°æ®é›†çš„å¤„ç†ä¼šè¢«ç¼“å­˜ï¼Œæ‰€ä»¥é‡æ–°æ‰§è¡Œä»£ç ï¼Œä¹Ÿä¸ä¼šå†è´¹æ—¶é—´ã€‚ï¼‰
- It can apply multiprocessing to go faster than applying the function on each element of the dataset.

    ï¼ˆå®ƒå¯ä»¥ä½¿ç”¨å¤šè¿›ç¨‹æ¥å¤„ç†ä»è€Œæé«˜å¤„ç†é€Ÿåº¦ã€‚ï¼‰
- It does not load the whole dataset into memory, saving the results as soon as one element is processed.

    ï¼ˆå®ƒä¸éœ€è¦æŠŠæ•´ä¸ªæ•°æ®é›†éƒ½åŠ è½½åˆ°å†…å­˜é‡Œï¼ŒåŒæ—¶æ¯ä¸ªå…ƒç´ ä¸€ç»å¤„ç†å°±ä¼šé©¬ä¸Šè¢«ä¿å­˜ï¼Œå› æ­¤ååˆ†èŠ‚çœå†…å­˜ã€‚ï¼‰

è§‚å¯Ÿä¸€ä¸‹ï¼Œè¿™é‡Œé€šè¿‡mapä¹‹åï¼Œå¾—åˆ°çš„Datasetçš„featureså˜å¤šäº†ï¼š
```python
features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids']
```
å¤šçš„å‡ ä¸ªcolumnså°±æ˜¯tokenizerå¤„ç†åçš„ç»“æœã€‚

æ³¨æ„åˆ°ï¼Œ**åœ¨è¿™ä¸ª`tokenize_function`ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨`padding`**ï¼Œå› ä¸ºå¦‚æœä½¿ç”¨äº†paddingä¹‹åï¼Œå°±ä¼šå…¨å±€ç»Ÿä¸€å¯¹ä¸€ä¸ªmaxlenè¿›è¡Œpaddingï¼Œè¿™æ ·æ— è®ºåœ¨tokenizeè¿˜æ˜¯æ¨¡å‹çš„è®­ç»ƒä¸Šéƒ½ä¸å¤Ÿé«˜æ•ˆã€‚



## Dynamic Padding åŠ¨æ€padding

å®é™…ä¸Šï¼Œæˆ‘ä»¬æ˜¯æ•…æ„å…ˆä¸è¿›è¡Œpaddingçš„ï¼Œå› ä¸ºæˆ‘ä»¬æƒ³**åœ¨åˆ’åˆ†batchçš„æ—¶å€™å†è¿›è¡Œpadding**ï¼Œè¿™æ ·å¯ä»¥é¿å…å‡ºç°å¾ˆå¤šæœ‰ä¸€å †paddingçš„åºåˆ—ï¼Œä»è€Œå¯ä»¥æ˜¾è‘—èŠ‚çœæˆ‘ä»¬çš„è®­ç»ƒæ—¶é—´ã€‚

è¿™é‡Œï¼Œæˆ‘ä»¬å°±éœ€è¦ç”¨åˆ°**`DataCollatorWithPadding`**ï¼Œæ¥è¿›è¡Œ**åŠ¨æ€padding**ï¼š


```python
from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

æ³¨æ„ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨tokenizeræ¥åˆå§‹åŒ–è¿™ä¸ª`DataCollatorWithPadding`ï¼Œå› ä¸ºéœ€è¦tokenizeræ¥å‘ŠçŸ¥å…·ä½“çš„padding tokenæ˜¯å•¥ï¼Œä»¥åŠpaddingçš„æ–¹å¼æ˜¯åœ¨å·¦è¾¹è¿˜æ˜¯å³è¾¹ï¼ˆä¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨çš„padding tokenä»¥åŠæ–¹å¼å¯èƒ½ä¸åŒï¼‰ã€‚


ä¸‹é¢å‡è®¾æˆ‘ä»¬è¦æä¸€ä¸ªsize=5çš„batchï¼Œçœ‹çœ‹å¦‚ä½•ä½¿ç”¨`DataCollatorWithPadding`æ¥å®ç°ï¼š


```python
samples = tokenized_datasets['train'][:5]
samples.keys()
# >>> ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids']
samples = {k:v for k,v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}  # æŠŠè¿™é‡Œå¤šä½™çš„å‡ åˆ—å»æ‰
samples.keys()
# >>> ['attention_mask', 'input_ids', 'label', 'token_type_ids']

# æ‰“å°å‡ºæ¯ä¸ªå¥å­çš„é•¿åº¦ï¼š
[len(x) for x in samples["input_ids"]]
```


```shell
>>>
[50, 59, 47, 67, 59]
```

ç„¶åæˆ‘ä»¬ä½¿ç”¨data_collatoræ¥å¤„ç†ï¼š


```python
batch = data_collator(samples)  # samplesä¸­å¿…é¡»åŒ…å« input_ids å­—æ®µï¼Œå› ä¸ºè¿™å°±æ˜¯collatorè¦å¤„ç†çš„å¯¹è±¡
batch.keys()
# >>> dict_keys(['attention_mask', 'input_ids', 'token_type_ids', 'labels'])

# å†æ‰“å°é•¿åº¦ï¼š
[len(x) for x in batch['input_ids']]
```


```shell
>>>
[67, 67, 67, 67, 67]
```



å¯ä»¥çœ‹åˆ°ï¼Œè¿™ä¸ª`data_collator`å°±æ˜¯ä¸€ä¸ªæŠŠç»™å®šdatasetè¿›è¡Œpaddingçš„å·¥å…·ï¼Œå…¶è¾“å…¥è·Ÿè¾“å‡ºæ˜¯å®Œå…¨ä¸€æ ·çš„æ ¼å¼ã€‚


```python
{k:v.shape for k,v in batch.items()}
```


```shell
>>>
{'attention_mask': torch.Size([5, 67]),
 'input_ids': torch.Size([5, 67]),
 'token_type_ids': torch.Size([5, 67]),
 'labels': torch.Size([5])}
```



è¿™ä¸ªbatchï¼Œå¯ä»¥å½¢æˆä¸€ä¸ªtensoräº†ï¼æ¥ä¸‹æ¥å°±å¯ä»¥ç”¨äºè®­ç»ƒäº†ï¼

---

å¯¹äº†ï¼Œè¿™é‡Œå¤šæä¸€å¥ï¼Œ`collator`è¿™ä¸ªå•è¯å®é™…ä¸Šåœ¨å¹³æ—¶ä½¿ç”¨è‹±è¯­çš„æ—¶å€™å¹¶ä¸å¸¸è§ï¼Œä½†å´åœ¨ç¼–ç¨‹ä¸­è§åˆ°å¤šæ¬¡ã€‚

æœ€å¼€å§‹ä¸€ç›´ä»¥ä¸ºæ˜¯`collector`ï¼Œæ„ä¸ºâ€œæ”¶é›†è€…â€ç­‰æ„æ€ï¼Œåæ¥æŸ¥äº†æŸ¥ï¼Œå‘ç°ä¸æ˜¯çš„ã€‚ä¸‹é¢æ˜¯æŸ¯æ—æ–¯è¯å…¸ä¸­å¯¹`collate`è¿™ä¸ªè¯çš„è§£é‡Šï¼š

> **collate**: 
>
> When you collate pieces of information, you **gather** them all together and **examine** them. 

å°±æ˜¯å½’çº³å¹¶æ•´ç†çš„æ„æ€ã€‚æ‰€ä»¥åœ¨æˆ‘ä»¬è¿™ä¸ªæƒ…æ™¯ä¸‹ï¼Œå°±æ˜¯å¯¹è¿™äº›æ‚ä¹±æ— ç« é•¿çŸ­ä¸ä¸€çš„åºåˆ—æ•°æ®ï¼Œè¿›è¡Œä¸€ä¸ªä¸ªåœ°åˆ†ç»„ï¼Œç„¶åæ£€æŸ¥å¹¶ç»Ÿä¸€é•¿åº¦ã€‚

å…³äºDataCollatoræ›´å¤šçš„ä¿¡æ¯ï¼Œå¯ä»¥å‚è§æ–‡æ¡£ï¼š
https://huggingface.co/transformers/master/main_classes/data_collator.html?highlight=datacollatorwithpadding#data-collator