---
title:  HuggingfaceğŸ¤—NLPç¬”è®°5ï¼šattention_maskåœ¨å¤„ç†å¤šä¸ªåºåˆ—æ—¶çš„ä½œç”¨
published: 2021-9-27
sidebar: auto
---

> **ã€ŒHuggingfaceğŸ¤—NLPç¬”è®°ç³»åˆ—-ç¬¬5é›†ã€**
> æœ€è¿‘è·Ÿç€Huggingfaceä¸Šçš„NLP tutorialèµ°äº†ä¸€éï¼ŒæƒŠå¹å±…ç„¶æœ‰å¦‚æ­¤å¥½çš„è®²è§£Transformersç³»åˆ—çš„NLPæ•™ç¨‹ï¼Œäºæ˜¯å†³å®šè®°å½•ä¸€ä¸‹å­¦ä¹ çš„è¿‡ç¨‹ï¼Œåˆ†äº«æˆ‘çš„ç¬”è®°ï¼Œå¯ä»¥ç®—æ˜¯å®˜æ–¹æ•™ç¨‹çš„ç²¾ç®€+æ³¨è§£ç‰ˆã€‚ä½†æœ€æ¨èçš„ï¼Œè¿˜æ˜¯ç›´æ¥è·Ÿç€å®˜æ–¹æ•™ç¨‹æ¥ä¸€éï¼ŒçœŸæ˜¯ä¸€ç§äº«å—ã€‚

- å®˜æ–¹æ•™ç¨‹ç½‘å€ï¼šhttps://huggingface.co/course/chapter1
- æœ¬æœŸå†…å®¹å¯¹åº”ç½‘å€ï¼šhttps://huggingface.co/course/chapter2/5?fw=pt
- æœ¬ç³»åˆ—ç¬”è®°çš„**GitHub**ï¼š https://github.com/beyondguo/Learn_PyTorch/tree/master/HuggingfaceNLP

---

# `attention_mask`åœ¨å¤„ç†å¤šä¸ªåºåˆ—æ—¶çš„ä½œç”¨

ç°åœ¨æˆ‘ä»¬è®­ç»ƒå’Œé¢„æµ‹åŸºæœ¬éƒ½æ˜¯æ‰¹é‡åŒ–å¤„ç†çš„ï¼Œè€Œå‰é¢å±•ç¤ºçš„ä¾‹å­å¾ˆå¤šéƒ½æ˜¯å•æ¡æ•°æ®ã€‚å•æ¡æ•°æ®è·Ÿå¤šæ¡æ•°æ®æœ‰ä¸€äº›éœ€è¦æ³¨æ„çš„åœ°æ–¹ã€‚

## å¤„ç†å•ä¸ªåºåˆ—

æˆ‘ä»¬é¦–å…ˆåŠ è½½ä¸€ä¸ªåœ¨æƒ…æ„Ÿåˆ†ç±»ä¸Šå¾®è°ƒè¿‡çš„æ¨¡å‹ï¼Œæ¥è¿›è¡Œæˆ‘ä»¬çš„å®éªŒï¼ˆæ³¨æ„ï¼Œè¿™é‡Œæˆ‘ä»¬å°±ä¸èƒ½èƒ½ä½¿ç”¨`AutoModel`ï¼Œè€Œåº”è¯¥ä½¿ç”¨`AutoModelFor*`è¿™ç§å¸¦Headçš„modelï¼‰ã€‚


```python
from pprint import pprint as print  # è¿™ä¸ªpprintèƒ½è®©æ‰“å°çš„æ ¼å¼æ›´å¥½çœ‹ä¸€ç‚¹
from transformers import AutoModelForSequenceClassification, AutoTokenizer
checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
```

å¯¹ä¸€ä¸ªå¥å­ï¼Œä½¿ç”¨tokenizerè¿›è¡Œå¤„ç†ï¼š


```python
s = 'Today is a nice day!'
inputs = tokenizer(s, return_tensors='pt')
print(inputs)
```

```shell
{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]]),
 'input_ids': tensor([[ 101, 2651, 2003, 1037, 3835, 2154,  999,  102]])}
```


å¯ä»¥çœ‹åˆ°ï¼Œè¿™é‡Œçš„inputsåŒ…å«äº†ä¸¤ä¸ªéƒ¨åˆ†ï¼š`input_ids`å’Œ`attention_mask`.

æ¨¡å‹å¯ä»¥ç›´æ¥æ¥å—`input_ids`ï¼š


```python
model(inputs.input_ids).logits
```

è¾“å‡ºï¼š


```shell
tensor([[-4.3232,  4.6906]], grad_fn=<AddmmBackward>)
```



ä¹Ÿå¯ä»¥é€šè¿‡`**inputs`åŒæ—¶æ¥å—`inputs`æ‰€æœ‰çš„å±æ€§ï¼š


```python
model(**inputs).logits
```

è¾“å‡ºï¼š


    tensor([[-4.3232,  4.6906]], grad_fn=<AddmmBackward>)



ä¸Šé¢ä¸¤ç§æ–¹å¼çš„**ç»“æœæ˜¯ä¸€æ ·çš„**ã€‚

## ä½†æ˜¯å½“æˆ‘ä»¬éœ€è¦åŒæ—¶å¤„ç†**å¤šä¸ªåºåˆ—**æ—¶ï¼Œæƒ…å†µå°±æœ‰å˜äº†ï¼


```python
ss = ['Today is a nice day!',
      'But what about tomorrow? Im not sure.']
inputs = tokenizer(ss, padding=True, return_tensors='pt')
print(inputs)
```
è¾“å‡ºï¼š
```shell
{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),
 'input_ids': tensor([[  101,  2651,  2003,  1037,  3835,  2154,   999,   102,     0,     0,
             0],
        [  101,  2021,  2054,  2055,  4826,  1029, 10047,  2025,  2469,  1012,
           102]])}
```


ç„¶åï¼Œæˆ‘ä»¬è¯•ç€ç›´æ¥æŠŠè¿™é‡Œçš„`input_ids`å–‚ç»™æ¨¡å‹


```python
model(inputs.input_ids).logits  # ç¬¬ä¸€ä¸ªå¥å­åŸæœ¬çš„logitsæ˜¯ [-4.3232,  4.6906]
```

è¾“å‡ºï¼š


```shell
tensor([[-4.1957,  4.5675],
        [ 3.9803, -3.2120]], grad_fn=<AddmmBackward>)
```



å‘ç°ï¼Œ**ç¬¬ä¸€ä¸ªå¥å­çš„`logits`å˜äº†**ï¼

è¿™æ˜¯**å› ä¸ºåœ¨paddingä¹‹åï¼Œç¬¬ä¸€ä¸ªå¥å­çš„encodingå˜äº†ï¼Œå¤šäº†å¾ˆå¤š0ï¼Œ è€Œself-attentionä¼šattendåˆ°æ‰€æœ‰çš„indexçš„å€¼ï¼Œå› æ­¤ç»“æœå°±å˜äº†**ã€‚

è¿™æ—¶ï¼Œå°±éœ€è¦æˆ‘ä»¬ä¸ä»…ä»…æ˜¯ä¼ å…¥`input_ids`ï¼Œè¿˜éœ€è¦ç»™å‡º`attention_mask`ï¼Œè¿™æ ·æ¨¡å‹å°±ä¼šåœ¨attentionçš„æ—¶å€™ï¼Œä¸å»attendè¢«maskæ‰çš„éƒ¨åˆ†ã€‚

å› æ­¤ï¼Œ**åœ¨å¤„ç†å¤šä¸ªåºåˆ—çš„æ—¶å€™ï¼Œæ­£ç¡®çš„åšæ³•æ˜¯ç›´æ¥æŠŠtokenizerå¤„ç†å¥½çš„ç»“æœï¼Œæ•´ä¸ªè¾“å…¥åˆ°æ¨¡å‹ä¸­**ï¼Œå³ç›´æ¥`**inputs`ã€‚
é€šè¿‡`**inputs`ï¼Œæˆ‘ä»¬å®é™…ä¸Šå°±æŠŠ`attention_mask`ä¹Ÿä¼ è¿›å»äº†:


```python
model(**inputs).logits
```

è¾“å‡ºï¼š


```shell
tensor([[-4.3232,  4.6906],
        [ 3.9803, -3.2120]], grad_fn=<AddmmBackward>)
```

ç°åœ¨ç¬¬ä¸€ä¸ªå¥å­çš„ç»“æœï¼Œå°±è·Ÿå‰é¢å•æ¡å¤„ç†æ—¶çš„ä¸€æ ·äº†ã€‚

