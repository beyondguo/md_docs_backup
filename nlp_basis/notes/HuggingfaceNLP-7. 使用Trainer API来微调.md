---
title:  HuggingfaceğŸ¤—NLPç¬”è®°7ï¼šä½¿ç”¨Trainer APIæ¥å¾®è°ƒæ¨¡å‹
published: 2021-9-28
sidebar: auto
---

> **ã€ŒHuggingfaceğŸ¤—NLPç¬”è®°ç³»åˆ—-ç¬¬7é›†ã€**
> æœ€è¿‘è·Ÿç€Huggingfaceä¸Šçš„NLP tutorialèµ°äº†ä¸€éï¼ŒæƒŠå¹å±…ç„¶æœ‰å¦‚æ­¤å¥½çš„è®²è§£Transformersç³»åˆ—çš„NLPæ•™ç¨‹ï¼Œäºæ˜¯å†³å®šè®°å½•ä¸€ä¸‹å­¦ä¹ çš„è¿‡ç¨‹ï¼Œåˆ†äº«æˆ‘çš„ç¬”è®°ï¼Œå¯ä»¥ç®—æ˜¯å®˜æ–¹æ•™ç¨‹çš„**ç²¾ç®€+æ³¨è§£ç‰ˆ**ã€‚ä½†æœ€æ¨èçš„ï¼Œè¿˜æ˜¯ç›´æ¥è·Ÿç€å®˜æ–¹æ•™ç¨‹æ¥ä¸€éï¼ŒçœŸæ˜¯ä¸€ç§äº«å—ã€‚

- å®˜æ–¹æ•™ç¨‹ç½‘å€ï¼šhttps://huggingface.co/course/chapter1
- æœ¬æœŸå†…å®¹å¯¹åº”ç½‘å€ï¼šhttps://huggingface.co/course/chapter3/3?fw=pt
- æœ¬ç³»åˆ—ç¬”è®°çš„**GitHub**ï¼š https://github.com/beyondguo/Learn_PyTorch/tree/master/HuggingfaceNLP

---



# ä½¿ç”¨Trainer APIæ¥å¾®è°ƒæ¨¡å‹

## 1. æ•°æ®é›†å‡†å¤‡å’Œé¢„å¤„ç†ï¼š

è¿™éƒ¨åˆ†å°±æ˜¯å›é¡¾ä¸Šä¸€é›†çš„å†…å®¹ï¼š
- é€šè¿‡datasetåŒ…åŠ è½½æ•°æ®é›†
- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œtokenizer
- å®šä¹‰Dataset.mapè¦ä½¿ç”¨çš„é¢„å¤„ç†å‡½æ•°
- å®šä¹‰DataCollatoræ¥ç”¨äºæ„é€ è®­ç»ƒbatch


```python
import numpy as np
from transformers import AutoTokenizer, DataCollatorWithPadding
import datasets
checkpoint = 'bert-base-cased'
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
raw_datasets = datasets.load_dataset('glue', 'mrpc')

def tokenize_function(sample):
    return tokenizer(sample['sentence1'], sample['sentence2'], truncation=True)
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```




## 2. åŠ è½½æˆ‘ä»¬è¦fine-tuneçš„æ¨¡å‹ï¼š


```python
from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

```shell
>>> (warnings)
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```


ä¸å¾—ä¸è¯´ï¼Œè¿™ä¸ªHuggingfaceå¾ˆè´´å¿ƒï¼Œè¿™é‡Œçš„warningå†™çš„å¾ˆæ¸…æ¥šã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯å¸¦`ForSequenceClassification`è¿™ä¸ªHeadçš„æ¨¡å‹ï¼Œä½†æ˜¯æˆ‘ä»¬çš„`bert-baed-cased`è™½ç„¶å®ƒæœ¬èº«ä¹Ÿæœ‰è‡ªèº«çš„Headï¼Œä½†è·Ÿæˆ‘ä»¬è¿™é‡Œçš„äºŒåˆ†ç±»ä»»åŠ¡ä¸åŒ¹é…ï¼Œæ‰€ä»¥å¯ä»¥çœ‹åˆ°ï¼Œå®ƒçš„Headè¢«ç§»é™¤äº†ï¼Œä½¿ç”¨äº†ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„`ForSequenceClassification`Headã€‚

æ‰€ä»¥è¿™é‡Œæç¤ºè¿˜è¯´ï¼š"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."

## 3. ä½¿ç”¨`Trainer`æ¥è®­ç»ƒ

`Trainer`æ˜¯Huggingface transformersåº“çš„ä¸€ä¸ªé«˜çº§APIï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬å¿«é€Ÿæ­å»ºè®­ç»ƒæ¡†æ¶ï¼š


```python
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(output_dir='test_trainer') # æŒ‡å®šè¾“å‡ºæ–‡ä»¶å¤¹ï¼Œæ²¡æœ‰ä¼šè‡ªåŠ¨åˆ›å»º

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,  # åœ¨å®šä¹‰äº†tokenizerä¹‹åï¼Œå…¶å®è¿™é‡Œçš„data_collatorå°±ä¸ç”¨å†å†™äº†ï¼Œä¼šè‡ªåŠ¨æ ¹æ®tokenizeråˆ›å»º
    tokenizer=tokenizer,
)
```

æˆ‘ä»¬çœ‹çœ‹`TrainingArguments`å’Œ`Trainer`çš„å‚æ•°éƒ½æœ‰äº›å•¥ï¼š

- https://huggingface.co/transformers/master/main_classes/trainer.html
- https://huggingface.co/transformers/master/main_classes/trainer.html#trainingarguments


```python
TrainingArguments(
    output_dir: Union[str, NoneType] = None,
    overwrite_output_dir: bool = False,
    do_train: bool = False,
    do_eval: bool = None,
    do_predict: bool = False,
    evaluation_strategy: transformers.trainer_utils.EvaluationStrategy = 'no',
    prediction_loss_only: bool = False,
    per_device_train_batch_size: int = 8,  # é»˜è®¤çš„batch_size=8
    per_device_eval_batch_size: int = 8,
    per_gpu_train_batch_size: Union[int, NoneType] = None,
    per_gpu_eval_batch_size: Union[int, NoneType] = None,
    gradient_accumulation_steps: int = 1,
    eval_accumulation_steps: Union[int, NoneType] = None,
    learning_rate: float = 5e-05,
    weight_decay: float = 0.0,
    adam_beta1: float = 0.9,
    adam_beta2: float = 0.999,
    adam_epsilon: float = 1e-08,
    max_grad_norm: float = 1.0,
    num_train_epochs: float = 3.0,   # é»˜è®¤è·‘3è½®
    ...
```

```python
Trainer(
    model: Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module] = None,
    args: transformers.training_args.TrainingArguments = None,
    data_collator: Union[DataCollator, NoneType] = None,
    train_dataset: Union[torch.utils.data.dataset.Dataset, NoneType] = None,
    eval_dataset: Union[torch.utils.data.dataset.Dataset, NoneType] = None,
    tokenizer: Union[ForwardRef('PreTrainedTokenizerBase'), NoneType] = None,
    model_init: Callable[[], transformers.modeling_utils.PreTrainedModel] = None,
    compute_metrics: Union[Callable[[transformers.trainer_utils.EvalPrediction], Dict], NoneType] = None,
    callbacks: Union[List[transformers.trainer_callback.TrainerCallback], NoneType] = None,
    optimizers: Tuple[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),  # é»˜è®¤ä¼šä½¿ç”¨AdamW
)
Docstring:     
Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for ğŸ¤— Transformers.
```

å¯è§ï¼Œè¿™ä¸ª`Trainer`æŠŠæ‰€æœ‰è®­ç»ƒä¸­éœ€è¦è€ƒè™‘çš„å‚æ•°ã€è®¾è®¡éƒ½åŒ…æ‹¬åœ¨å†…äº†ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡ŒæŒ‡å®šè®­ç»ƒéªŒè¯é›†ã€data_collatorã€metricsã€optimizerï¼Œå¹¶é€šè¿‡`TrainingArguments`æ¥æä¾›å„ç§è¶…å‚æ•°ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œ`Trainer`å’Œ`TrainingArguments`ä¼šä½¿ç”¨ï¼š
- batch size=8
- epochs = 3
- AdamWä¼˜åŒ–å™¨


å®šä¹‰å¥½ä¹‹åï¼Œç›´æ¥ä½¿ç”¨`.train()`æ¥å¯åŠ¨è®­ç»ƒï¼š


```python
trainer.train()
```

è¾“å‡ºï¼š

![image-20210927151316532](https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20210927151316.png)

```shell
TrainOutput(global_step=1377, training_loss=0.35569445984728887, metrics={'train_runtime': 383.0158, 'train_samples_per_second': 3.595, 'total_flos': 530185443455520, 'epoch': 3.0})
```



ç„¶åæˆ‘ä»¬ç”¨`Trainer`æ¥é¢„æµ‹ï¼š

`trainer.predict()`å‡½æ•°å¤„ç†çš„ç»“æœæ˜¯ä¸€ä¸ª`named_tuple`ï¼ˆä¸€ç§å¯ä»¥ç›´æ¥é€šè¿‡keyæ¥å–å€¼çš„tupleï¼‰ï¼Œç±»ä¼¼ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ä¸‰ä¸ªå±æ€§ï¼špredictions, label_ids, metrics

æ³¨æ„ï¼Œè¿™é‡Œçš„ä¸‰ä¸ªå±æ€§ï¼š
- `predictions`å®é™…ä¸Šå°±æ˜¯logits
- `label_ids`ä¸æ˜¯é¢„æµ‹å‡ºæ¥çš„idï¼Œè€Œæ˜¯æ•°æ®é›†ä¸­è‡ªå¸¦çš„ground truthçš„æ ‡ç­¾ï¼Œå› æ­¤å¦‚æœè¾“å…¥çš„æ•°æ®é›†ä¸­æ²¡ç»™æ ‡ç­¾ï¼Œè¿™é‡Œä¹Ÿä¸ä¼šè¾“å‡º
- `metrics`ï¼Œä¹Ÿæ˜¯åªæœ‰è¾“å…¥çš„æ•°æ®é›†ä¸­æä¾›äº†`label_ids`æ‰ä¼šè¾“å‡ºmetricsï¼ŒåŒ…æ‹¬lossä¹‹ç±»çš„æŒ‡æ ‡

å…¶ä¸­`metrics`ä¸­è¿˜å¯ä»¥åŒ…å«æˆ‘ä»¬è‡ªå®šä¹‰çš„å­—æ®µï¼Œæˆ‘ä»¬éœ€è¦åœ¨å®šä¹‰`Trainer`çš„æ—¶å€™ç»™å®š`compute_metrics`å‚æ•°ã€‚

æ–‡æ¡£å‚è€ƒï¼š https://huggingface.co/transformers/master/main_classes/trainer.html#transformers.Trainer.predict


```python
predictions = trainer.predict(tokenized_datasets['validation'])
print(predictions.predictions.shape)  # logits
# array([[-2.7887206,  3.1986978],
#       [ 2.5258656, -1.832253 ], ...], dtype=float32)
print(predictions.label_ids.shape) # array([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, ...], dtype=int64)
print(predictions.metrics)
```


è¾“å‡ºï¼š

```shell
[51/51 00:03]
(408, 2)
(408,)
{'eval_loss': 0.7387174963951111, 'eval_runtime': 3.2872, 'eval_samples_per_second': 124.117}
```


ç„¶åå°±å¯ä»¥ç”¨predså’Œlabelsæ¥è®¡ç®—ä¸€äº›ç›¸å…³çš„metricsäº†ã€‚

Huggingface `datasets`é‡Œé¢å¯ä»¥ç›´æ¥å¯¼å…¥è·Ÿæ•°æ®é›†ç›¸å…³çš„metricsï¼š


```python
from datasets import load_metric

preds = np.argmax(predictions.predictions, axis=-1)

metric = load_metric('glue', 'mrpc')
metric.compute(predictions=preds, references=predictions.label_ids)
```


```shell
>>>
{'accuracy': 0.8455882352941176, 'f1': 0.8911917098445595}
```

çœ‹çœ‹è¿™é‡Œçš„metricï¼ˆglue typeï¼‰çš„æ–‡æ¡£ï¼š
```shell
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
```



## 4.æ„å»º`Trainer`ä¸­çš„`compute_metrics`å‡½æ•°

å‰é¢æˆ‘ä»¬æ³¨æ„åˆ°`Trainer`çš„å‚æ•°ä¸­ï¼Œå¯ä»¥æä¾›ä¸€ä¸ª`compute_metrics`å‡½æ•°ï¼Œç”¨äºè¾“å‡ºæˆ‘ä»¬å¸Œæœ›æœ‰çš„ä¸€äº›æŒ‡æ ‡ã€‚

è¿™ä¸ª`compute_metrics`æœ‰ä¸€äº›è¾“å…¥è¾“å‡ºçš„è¦æ±‚ï¼š
- è¾“å…¥ï¼šæ˜¯ä¸€ä¸ª`EvalPrediction`å¯¹è±¡ï¼Œæ˜¯ä¸€ä¸ªnamed tupleï¼Œéœ€è¦æœ‰è‡³å°‘`predictions`å’Œ`label_ids`ä¸¤ä¸ªå­—æ®µï¼›ç»è¿‡æŸ¥çœ‹æºç ï¼Œè¿™é‡Œçš„predictionsï¼Œ**å°±æ˜¯logits**
- è¾“å‡ºï¼šä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«å„ä¸ªmetricså’Œå¯¹åº”çš„æ•°å€¼ã€‚

æºç åœ°å€ï¼š https://huggingface.co/transformers/master/_modules/transformers/trainer.html#Trainer


```python
from datasets import load_metric
def compute_metrics(eval_preds):
    metric = load_metric("glue", "mrpc")
    logits, labels = eval_preds.predictions, eval_preds.label_ids
    # ä¸Šä¸€è¡Œå¯ä»¥ç›´æ¥ç®€å†™æˆï¼š
    # logits, labels = eval_preds  å› ä¸ºå®ƒç›¸å½“äºä¸€ä¸ªtuple
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

### æ€»ç»“ä¸€ä¸‹è¿™ä¸ªè¿‡ç¨‹ï¼š

- é¦–å…ˆæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ª`compute_metrics`å‡½æ•°ï¼Œäº¤ç»™`Trainer`ï¼›
- `Trainer`è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹ä¼šå¯¹æ ·æœ¬è®¡ç®—ï¼Œäº§ç”Ÿ predictions (logits)ï¼›
- `Trainer`å†æŠŠ predictions å’Œæ•°æ®é›†ä¸­ç»™å®šçš„ label_ids æ‰“åŒ…æˆä¸€ä¸ªå¯¹è±¡ï¼Œå‘é€ç»™`compute_metrics`å‡½æ•°ï¼›
- `compute_metrics`å‡½æ•°è®¡ç®—å¥½ç›¸åº”çš„ metrics ç„¶åè¿”å›ã€‚

## çœ‹çœ‹å¸¦ä¸Šäº† compute_metrics ä¹‹åçš„è®­ç»ƒï¼š


```python
training_args = TrainingArguments(output_dir='test_trainer', evaluation_strategy='epoch')
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)  # new model
trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,  # åœ¨å®šä¹‰äº†tokenizerä¹‹åï¼Œå…¶å®è¿™é‡Œçš„data_collatorå°±ä¸ç”¨å†å†™äº†ï¼Œä¼šè‡ªåŠ¨æ ¹æ®tokenizeråˆ›å»º
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()
```

è¾“å‡ºï¼š

![image-20210927151216236](https://gitee.com/beyond_guo/typora_pics/raw/master/typora/20210927151222.png)

```shell
TrainOutput(global_step=1377, training_loss=0.32063739751678666, metrics={'train_runtime': 414.1719, 'train_samples_per_second': 3.325, 'total_flos': 530351810395680, 'epoch': 3.0})
```



å¯è§ï¼Œå¸¦ä¸Šäº†`compute_metircs`å‡½æ•°ä¹‹åï¼Œåœ¨Trainerè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¼šæŠŠå¢åŠ çš„metricä¹Ÿæ‰“å°å‡ºæ¥ï¼Œæ–¹ä¾¿æˆ‘ä»¬æ—¶åˆ»äº†è§£è®­ç»ƒçš„è¿›å±•ã€‚

