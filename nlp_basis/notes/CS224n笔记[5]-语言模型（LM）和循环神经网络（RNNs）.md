---
title: CS224n笔记[5]:语言模型（LM）和循环神经网络（RNNs）
published: 2021-6-24
sidebar: auto
---

# CS224n笔记[5]:语言模型（LM）和循环神经网络（RNNs）

<center>作者：郭必扬</center>

许久没更新了，十分惭愧，翻了翻之前的笔记，才之前上一期我们讲的是“依存分析”。本期，我们介绍一下语言模型（Language Model）和循环神经网络（RNNs）的基本知识。

## 语言模型

语言模型（language models）是NLP的基础技术之一，这个名字听起来总是很玄乎，又因为其不想文本分类、实体识别这些技术这么常用，导致包括我在内的很多人一直对这个玩意儿一知半解。所以今天我们就来一起跟着CS224N网课把这个概念搞清楚。

理解玄乎的理论的最佳方法，就是记住它的一些经典的具体应用。语言模型最典型的应用就是“输入法联想”。
比如我在搜狗输入法里写一个“我”，输入法会给我推荐几个词：“女朋友”、“刚刚”、“的”等等。


![](https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-24/1624548168728-image.png)


为啥会推荐这些词呢，可能是搜狗对我曾经输入过的成百上千句话训练过语言模型，知道了我输入“我”之后，很大概率会接着输入“女朋友”，也有很大概率输入“刚刚”，因此输入法就把这些词给排到了前面。
在一些社交平台上，我们也会经常玩一个游戏：“给定一个初始词，不断通过自己输入法的联想词来造句，看看会造出什么句子来”，往往我们发现可以构成一个完整的说得通的句子，甚至还能暴露出我们个人的一些习惯。

这些我们每天都在使用的功能，实际上都在默默地使用语言模型的技术。下面我们来给语言模型下一个定义。

语言模型其实可以从两个角度去理解，因此我们给出两种定义：
>定义一：语言模型（LM）的任务就是预测一段文字接下来会出现什么词。

即一个语言模型应该有能力计算下面这个公式的值：
$$
P(x^{(t+1)}|x^{(t)},...,x^{(1)})
$$
翻译过来就是，在已知一句话的前t个词的基础上，通过LM可以计算出下一个词是某个词的概率。

>定义二：语言模型（LM）给一段文本赋予一个概率。

即对于一段文字 $x^{(1)},x^{(2)},...,x^{(T)}$，LM可可以计算出这句话出现的概率：
$$
\begin{align}
&P(x^{(1)},...,x^{(T)}) \nonumber \\
&=P(x^{(1)})\cdot P(x^{(2)}|x^{(1)}) \cdot P(x^{(3)}|x^{(2)},x^{(1)})... \nonumber \\
&=\Pi^{T}_{t=1}P(x^{(t)}|x^{(t-1)},...,x^{(1)})  \nonumber \\
\end{align}
$$

其中
$$P(x^{(t)}|x^{(t-1)},...,x^{(1)})$$
就是LM可以计算出来的。

所以回头看，这两种定义是一回事儿。


## 如何学习语言模型
首先再介绍一个概念：N-gram。
所谓的N-gram，就是指一堆连续的词，根据这一堆词的个数，我们可以分为unigram,bigram,trigram等等。

如果我们需要得到一个N-gram的LM，它的意思就是希望我们可以通过N-1个词预测第N个词的概率。

那么如何学习得到一个N-gram的LM呢？一个直接的思路就是，我们可以收集关于语料中的各个N-gram出现的频率信息。

对于一个N-gram的LM，我们需要做一个假设：

>Assumption：某个词出现的概率只由其前N-1个词决定。

比如我们想得到一个3-gram的LM，那么就是说想预测一段文本的下一个词是什么的话，只用看这个词前面2个词即可。

还是用输入法联想中的例子：


![](https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-24/1624548188319-image.png)


要根据“罗永浩是什么”的下一个字是什么，如果我们设定的是3-gram，则我们只使用图中蓝色的“什么”二字来预测，而不看前面黑色的字。

N-gram的LM用公式表达即为：
$$
\begin{align}
&P(x^{(t)}|x^{(t-1)},...,x^{(1)}) \nonumber \\
&=P(x^{(t)}|x^{(t-1)},...,x^{(t-N+1)})(1) \nonumber \\
&=\frac{P(x^{(t)},x^{(t-1)},...,x^{(t-N+1)})}{P(x^{(t-1)},...,x^{(t-N+1)})} (2) \nonumber
\end{align}
$$
公式的第一个等式，就是使用了我们前面的假设，第二个等式就是条件概率的计算方法。
(2)式的分子就是代表这个由N个词构成的N-gram出现的概率，分母则是一个(N-1)-gram出现的概率。

如何计算得到这些概率值呢？————数它就完事儿了！
我们就在语料中去count，用频率来估计这些概率。

**显然，由于这是一个分式，会有分母分子为0的可能，这些特殊情况是我们需要考虑的：**

比如分子，它为0的可能性其实很大，因为随机给定一个N-gram，它真的会在语料中出现的概率其实很小，多数可能都不存在，这个时候，我们就需要使用一个很小的数来补到分子上。

分母是一个(N-1)-gram，也很有可能不存在，导致分母为0，这个时候，我们就采用回退（back-off）的策略，转而统计(N-2)-gram的个数，N越小，其出现的概率实际上越大，所以不断回退，总可以找到不为0的情况。

通过上面的说明，我们发现学习一个N-gram的LM其实很简单，就是数数问题。数完了这些数儿，就得到了LM。但是从上面的分析过程，我们也不难发现这样得到的LM的一些严重问题：

1. **稀疏性问题**。我们前面提到过分子分母很容易为0，就是由于N-gram的稀疏性造成的，N越大，这种稀疏性的问题就越严重，很可能你统计的大多数N-gram都不存在。
2. **存储问题**。虽然这种基于计数的LM的很简单，但是我们必须穷举出预料中所有可能的N-gram，并逐一去计数、保存，N一旦大起来的话，模型的大小就会陡增。
3. N太大了会有过于稀疏、难以保存的问题，相反，N太小了，模型的预测**准确率**就会明显降低。

我们知道，语言模型最直接的用途，就是文本联想，和文本生成了。
对于文本联想，这种基于计数的LM也不是不能用，毕竟联想出来的词确实在统计意义上是更加频繁的，所以用户直接感受不出它的缺点。

但对于文本生成来说，这种基于计数的LM则有很大问题了。这里拿CS224N的PPT上的一个例子来说明：


![](https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-24/1624548203839-image.png)


这段文字中高亮的"today the"是预先给定的文本，后面则是通过LM一个字一个字预测出来形成的文本。
这段文本，牛逼之处在于，它的语法基本没啥错，可以我们读完之后，完全不知道在讲啥，因为它说着说着就说偏了，再说一会又偏了，导致整个文本没有一个清晰的主题。很明显，这里训练的LM采用的N-gram一定不大。但是N一大起来，训练起来又很困难。这就是基于计数的LM的困境。

不过，这玩意儿来做一个bullshit-generator还是挺不错的！


## RNN和基于RNN的语言模型

前面讲到的基于计数的LM的缺点，主要就是由它的N-gram的N不能太大又不能太小的限制造成的。
如果能有一个结构，可以处理任意长度的输入，而不是要固定一长度的话，那就可以解决这个问题了。

在这样的启发下，RNN被提出了。

一个经典的RNN结构可以这样表示：


![](https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-24/1624548218075-image.png)


RNN即循环神经网络，为何叫循环呢？因为不管RNN有多长，它实际上都是在**同一个神经网络中不断循环**，例如图中话的4个隐层神经网络，实际上都是同一个，因此他们的**权重都是一样**的，只是根据输入的不同，而产生不同的输出。

有了这样的结构，使得RNN具有以下这些优点：

1. 可以处理任意长度的输入
2. 模型的大小不随输入长度的增加而增大
3. 后面步骤的处理过程可以利用到前面的信息
4. 每一步具有相同的权重矩阵，使得网络可以利用不同输入的相似性

然而，RNN也有其缺点：
1. 计算慢。这是由于它必须在处理完上一步后才能进行下一步的计算。
2. 当输入过长的时候，难以捕捉长距离依赖。

当然，这些问题都是后话了，后面我们学习的GRU，LSTM乃至Transformer都是在不断改进RNN的种种缺点。

### 如何训练一个基于RNN的LM呢？
首先，我们收集一大批语料，这些语料是由很多个序列（句子、短语等等）组成的。
我们把序列作为输入，输入到RNN网络中，每一步都可以得到一个输出，这个输出即为当前步的下一个词的概率分布。我们使用这个概率分布可以和真实的概率分布（其实就是一个one-hot向量）计算一个损失。明确了损失函数，我们就很容易去训练了。CS224N中的一张图描绘地很清晰：


![](https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/2021-6-24/1624548234435-image.png)

一图胜前言，这里就不再赘述了。

### 训练好RNN了之后，如何进行文本生成呢？
输入一个词，每一步的输出都作为下一步的输入，这样就可以通过一个词不断进行文本生成了。


### 如何评价一个语言模型呢？
使用**Perplexity**(困惑度)：

这个公式计算出来的结果越大，就说明你这个模型越让人“困惑”，也就是不好了。

通过公式的变形，我们可以发现这个困惑度，等价于交叉熵损失函数：
$$
\Pi^{T}_{t=1}(\frac{1}{\hat{y}^{t}})^{\frac{1}{T}}=exp(-\frac{1}{T}\sum^{T}_{t=1}log(\hat{y}^{t}))
$$
所以，我们在优化RNN LM的时候，就是在使劲降低perplexity。


## 为何语言模型很重要
语言模型（LM）对于许多学习NLP的同学来说，是一个熟悉的陌生人，因为很多时候我们并不会去专门做一个语言模型，不像文本分类这种任务，任何学习NLP的同学都会去通过文本分类来练手。

但语言模型依然是十分重要的，主要体现在下面两个方面：
1. 它是检测NLP模型是否理解了语言的一个基准任务（benchmark task）；
2. 它是很多NLP任务的基础子模块，尤其是涉及到文本生成、文本联想的任务。例如输入法预测、拼写检查、语音识别、手写文字识别、机器翻译、摘要生成、写作风格鉴定等等。随着NLP学习的深入，我们会越来越发现LM的重要性。



