---
title:  HuggingfaceğŸ¤—NLPç¬”è®°8ï¼šä½¿ç”¨PyTorchæ¥å¾®è°ƒæ¨¡å‹
published: 2021-10-01
sidebar: auto
---

> **ã€ŒHuggingfaceğŸ¤—NLPç¬”è®°ç³»åˆ—-ç¬¬8é›†ã€**
> Huggingfaceåˆçº§æ•™ç¨‹å®Œç»“æ’’èŠ±ï¼ğŸŒ¸ğŸŒ¼ãƒ½(Â°â–½Â°)ãƒğŸŒ¸ğŸŒº
> æœ€è¿‘è·Ÿç€Huggingfaceä¸Šçš„NLP tutorialèµ°äº†ä¸€éï¼ŒæƒŠå¹å±…ç„¶æœ‰å¦‚æ­¤å¥½çš„è®²è§£Transformersç³»åˆ—çš„NLPæ•™ç¨‹ï¼Œäºæ˜¯å†³å®šè®°å½•ä¸€ä¸‹å­¦ä¹ çš„è¿‡ç¨‹ï¼Œåˆ†äº«æˆ‘çš„ç¬”è®°ï¼Œå¯ä»¥ç®—æ˜¯å®˜æ–¹æ•™ç¨‹çš„**ç²¾ç®€+æ³¨è§£ç‰ˆ**ã€‚ä½†æœ€æ¨èçš„ï¼Œè¿˜æ˜¯ç›´æ¥è·Ÿç€å®˜æ–¹æ•™ç¨‹æ¥ä¸€éï¼ŒçœŸæ˜¯ä¸€ç§äº«å—ã€‚

- å®˜æ–¹æ•™ç¨‹ç½‘å€ï¼šhttps://huggingface.co/course/chapter1
- æœ¬æœŸå†…å®¹å¯¹åº”ç½‘å€ï¼šhttps://huggingface.co/course/chapter3/4?fw=pt
- æœ¬ç³»åˆ—ç¬”è®°çš„**GitHub Notebook(å¯ä¸‹è½½ç›´æ¥è¿è¡Œ)**ï¼š https://github.com/beyondguo/Learn_PyTorch/tree/master/HuggingfaceNLP

---


# æ›´åŠ é€æ˜çš„æ–¹å¼â€”â€”ä½¿ç”¨PyTorchæ¥å¾®è°ƒæ¨¡å‹

è¿™é‡Œæˆ‘ä»¬ä¸ä½¿ç”¨Trainerè¿™ä¸ªé«˜çº§APIï¼Œè€Œæ˜¯ç”¨pytorchæ¥å®ç°ã€‚


## 1. æ•°æ®é›†é¢„å¤„ç†
åœ¨Huggingfaceå®˜æ–¹æ•™ç¨‹é‡Œæåˆ°ï¼Œåœ¨ä½¿ç”¨pytorchçš„dataloaderä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦åšä¸€äº›äº‹æƒ…ï¼š
- æŠŠdatasetä¸­ä¸€äº›ä¸éœ€è¦çš„åˆ—ç»™å»æ‰äº†ï¼Œæ¯”å¦‚â€˜sentence1â€™ï¼Œâ€˜sentence2â€™ç­‰
- æŠŠæ•°æ®è½¬æ¢æˆpytorch tensors
- ä¿®æ”¹åˆ—å label ä¸º labels

å…¶ä»–çš„éƒ½å¥½è¯´ï¼Œä½†**ä¸ºå•¥è¦ä¿®æ”¹åˆ—å label ä¸º labelsï¼Œå¥½å¥‡æ€ªå“¦ï¼**
è¿™é‡Œæ¢ç©¶ä¸€ä¸‹ï¼š

é¦–å…ˆï¼ŒHuggingfaceçš„è¿™äº›transformer Modelç›´æ¥callçš„æ—¶å€™ï¼Œæ¥å—çš„æ ‡ç­¾è¿™ä¸ªå‚æ•°æ˜¯å«"labels"ã€‚
æ‰€ä»¥ä¸ç®¡ä½ ä½¿ç”¨Trainerï¼Œè¿˜æ˜¯åŸç”Ÿpytorchå»å†™ï¼Œæœ€ç»ˆæ¨¡å‹å¤„ç†çš„æ—¶å€™ï¼Œè‚¯å®šæ˜¯ä½¿ç”¨çš„åä¸º"labels"çš„æ ‡ç­¾å‚æ•°ã€‚


ä½†åœ¨Huggingfaceçš„datasetsä¸­ï¼Œæ•°æ®é›†çš„æ ‡ç­¾ä¸€èˆ¬å‘½åä¸º"label"æˆ–è€…"label_ids"ï¼Œé‚£ä¸ºä»€ä¹ˆåœ¨å‰ä¸¤é›†ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰å¯¹æ ‡ç­¾åè¿›è¡Œå¤„ç†å‘¢ï¼Ÿ

è¿™ä¸€ç‚¹åœ¨transformerçš„æºç `trainer.py`é‡Œæ‰¾åˆ°äº†ç«¯å€ªï¼š
```python
# ä½ç½®åœ¨def _remove_unused_columnså‡½æ•°é‡Œ
# Labels may be named label or label_ids, the default data collator handles that.
signature_columns += ["label", "label_ids"]
```
è¿™é‡Œæç¤ºäº†ï¼Œ data collator ä¼šè´Ÿè´£å¤„ç†æ ‡ç­¾é—®é¢˜ã€‚ç„¶åæˆ‘åˆå»æŸ¥çœ‹äº†`data_collator.py`ä¸­å‘ç°äº†ä¸€ä¸‹å†…å®¹ï¼š
```python
class DataCollatorWithPadding:
    ...
    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        ...
        if "label" in batch:
            batch["labels"] = batch["label"]
            del batch["label"]
        if "label_ids" in batch:
            batch["labels"] = batch["label_ids"]
            del batch["label_ids"]
        return batch
```
è¿™å°±çœŸç›¸å¤§ç™½äº†ï¼šä¸ç®¡æ•°æ®é›†ä¸­æä¾›çš„æ ‡ç­¾åå«"label"ï¼Œè¿˜æ˜¯"label_ids"ï¼Œ
DataCollatorWithPadding éƒ½ä¼šå¸®ä½ è½¬æ¢æˆ"labels"ï¼Œè£…è¿›batché‡Œï¼Œå†è¿”å›ã€‚

å‰é¢ä½¿ç”¨Trainerçš„æ—¶å€™ï¼ŒDataCollatorWithPaddingå·²ç»å¸®æˆ‘ä»¬è‡ªåŠ¨è½¬æ¢äº†ï¼Œå› æ­¤æˆ‘ä»¬ä¸éœ€è¦æ“å¿ƒè¿™ä¸ªé—®é¢˜ã€‚

ä½†è¿™å°±æ˜¯è®©æˆ‘ç–‘æƒ‘çš„åœ°æ–¹ï¼šæˆ‘ä»¬ä½¿ç”¨pytorchæ¥å†™ï¼Œå…¶å®ä¹Ÿä¸ç”¨ç®¡è¿™ä¸ªï¼Œå› ä¸ºåœ¨pytorchçš„data_loaderé‡Œé¢ï¼Œæœ‰ä¸€ä¸ª`collate_fn`å‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠDataCollatorWithPaddingå¯¹è±¡ä¼ è¿›å»ï¼Œä¹Ÿä¼šå¸®æˆ‘ä»¬è‡ªåŠ¨æŠŠ"label"è½¬æ¢æˆ"labels"ã€‚å› æ­¤å®é™…ä¸Šï¼Œ**è¿™åº”è¯¥æ˜¯æ•™ç¨‹ä¸­çš„ä¸€ä¸ªå°é”™è¯¯ï¼Œæˆ‘ä»¬ä¸éœ€è¦æ‰‹åŠ¨è®¾è®¡**ï¼ˆå‰ä¸¤å¤©åœ¨Huggingface GitHubä¸Šæäº†issueï¼Œä½œè€…å›å¤æˆ‘ç¡®å®ä¸ç”¨æ‰‹åŠ¨è®¾ç½®ï¼‰ã€‚

---

ä¸‹é¢å¼€å§‹æ­£å¼ä½¿ç”¨pytorchæ¥è®­ç»ƒï¼š

é¦–å…ˆæ˜¯è·Ÿä¹‹å‰ä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦åŠ è½½æ•°æ®é›†ã€tokenizerï¼Œç„¶åæŠŠæ•°æ®é›†é€šè¿‡mapçš„æ–¹å¼è¿›è¡Œé¢„å¤„ç†ã€‚æˆ‘ä»¬è¿˜éœ€è¦å®šä¹‰ä¸€ä¸ª`data_collator`æ–¹ä¾¿æˆ‘ä»¬åé¢è¿›è¡Œæ‰¹é‡åŒ–å¤„ç†æ¨¡å‹ï¼š


```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

æŸ¥çœ‹ä¸€ä¸‹å¤„ç†åçš„datasetï¼š

```python
print(tokenized_datasets['train'].column_names)
```

```shell
>>>
['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids']
```


huggingface datasetsè´´å¿ƒåœ°å‡†å¤‡äº†ä¸‰ä¸ªæ–¹æ³•ï¼š`remove_columns`, `rename_column`, `set_format`

æ¥æ–¹ä¾¿æˆ‘ä»¬ä¸ºpytorchçš„Dataloaderåšå‡†å¤‡ï¼š


```python
tokenized_datasets = tokenized_datasets.remove_columns(['sentence1', 'sentence2','idx'])
# tokenized_datasets = tokenized_datasets.rename_column('label','labels')  # å®è·µè¯æ˜ï¼Œè¿™ä¸€è¡Œæ˜¯ä¸éœ€è¦çš„
tokenized_datasets.set_format('torch')

print(tokenized_datasets['train'].column_names)
```

```shell
>>>
['attention_mask', 'input_ids', 'label', 'token_type_ids']
```

æŸ¥çœ‹ä¸€ä¸‹ï¼š

```python
tokenized_datasets['train']  # ç»è¿‡ä¸Šé¢çš„å¤„ç†ï¼Œå®ƒå°±å¯ä»¥ç›´æ¥ä¸¢è¿›pytorchçš„Dataloaderä¸­äº†ï¼Œè·Ÿpytorchä¸­çš„Datasetæ ¼å¼å·²ç»ä¸€æ ·äº†
```


```shell
>>>
Dataset({
    features: ['attention_mask', 'input_ids', 'label', 'token_type_ids'],
    num_rows: 3668
})
```



å®šä¹‰æˆ‘ä»¬çš„**pytorch dataloaders**ï¼š

åœ¨pytorchçš„`DataLoader`é‡Œï¼Œæœ‰ä¸€ä¸ª`collate_fn`å‚æ•°ï¼Œå…¶å®šä¹‰æ˜¯ï¼š"merges a list of samples to form a mini-batch of Tensor(s).  Used when using batched loading from a map-style dataset." æˆ‘ä»¬å¯ä»¥ç›´æ¥æŠŠHuggingfaceçš„`DataCollatorWithPadding`å¯¹è±¡ä¼ è¿›å»ï¼Œç”¨äºå¯¹æ•°æ®è¿›è¡Œpaddingç­‰ä¸€ç³»åˆ—å¤„ç†ï¼š


```python
from torch.utils.data import DataLoader, Dataset
train_dataloader = DataLoader(tokenized_datasets['train'], shuffle=True, batch_size=8, collate_fn=data_collator)  # é€šè¿‡è¿™é‡Œçš„dataloaderï¼Œæ¯ä¸ªbatchçš„seq_lenå¯èƒ½ä¸åŒ
eval_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=8, collate_fn=data_collator)
```


```python
# æŸ¥çœ‹ä¸€ä¸‹train_dataloaderçš„å…ƒç´ é•¿å•¥æ ·
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
# å¯è§éƒ½æ˜¯é•¿åº¦ä¸º72ï¼Œsize=8çš„batch
```


```shell
>>>
{'attention_mask': torch.Size([8, 72]),
 'input_ids': torch.Size([8, 72]),
 'token_type_ids': torch.Size([8, 72]),
 'labels': torch.Size([8])}
```

è§‚å¯Ÿä¸€ä¸‹ç»è¿‡DataLoaderå¤„ç†åçš„æ•°æ®ï¼Œæˆ‘ä»¬å‘ç°ï¼Œæ ‡ç­¾é‚£ä¸€åˆ—çš„åˆ—åï¼Œå·²ç»ä»`"label"`å˜ä¸º`"labels"`äº†ï¼

## 2. æ¨¡å‹


```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

å‰é¢dataloaderå‡ºæ¥çš„batchå¯ä»¥ç›´æ¥ä¸¢è¿›æ¨¡å‹å¤„ç†ï¼š

```python
model(**batch) 
```


```shell
>>>
SequenceClassifierOutput(loss=tensor(0.7563, grad_fn=<NllLossBackward>), logits=tensor([[-0.2171, -0.4416],
        [-0.2248, -0.4694],
        [-0.2440, -0.4664],
        [-0.2421, -0.4510],
        [-0.2273, -0.4545],
        [-0.2339, -0.4515],
        [-0.2334, -0.4387],
        [-0.2362, -0.4601]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)
```



## å®šä¹‰ optimizer å’Œ learning rate scheduler

æŒ‰é“ç†è¯´ï¼ŒHuggingfaceè¿™è¾¹æä¾›Transformeræ¨¡å‹å°±å·²ç»å¤Ÿäº†ï¼Œå…·ä½“çš„è®­ç»ƒã€ä¼˜åŒ–ï¼Œåº”è¯¥äº¤ç»™pytorchäº†å§ã€‚ä½†é‰´äºTransformerè®­ç»ƒæ—¶ï¼Œæœ€å¸¸ç”¨çš„ä¼˜åŒ–å™¨å°±æ˜¯AdamWï¼Œè¿™é‡ŒHuggingfaceä¹Ÿç›´æ¥åœ¨`transformers`åº“ä¸­åŠ å…¥äº†`AdamW`è¿™ä¸ªä¼˜åŒ–å™¨ï¼Œè¿˜è´´å¿ƒåœ°é…å¤‡äº†lr_schedulerï¼Œæ–¹ä¾¿æˆ‘ä»¬ç›´æ¥ä½¿ç”¨ã€‚


```python
from transformers import AdamW, get_scheduler

optimizer = AdamW(model.parameters(), lr=5e-5)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)  # num of batches * num of epochs
lr_scheduler = get_scheduler(
    'linear',
    optimizer=optimizer,  # scheduleræ˜¯é’ˆå¯¹optimizerçš„lrçš„
    num_warmup_steps=0,
    num_training_steps=num_training_steps)
print(num_training_steps)
```

    1377


## 3. Training

é¦–å…ˆï¼Œæˆ‘ä»¬è®¾ç½®cuda deviceï¼Œç„¶åæŠŠæ¨¡å‹ç»™ç§»åŠ¨åˆ°cudaä¸Šï¼š


```python
import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
```



## ç¼–å†™pytorch training loops:

è¿™é‡Œä¹Ÿå¾ˆç®€å•ï¼Œæ€è·¯å°±æ˜¯è¿™æ ·ï¼š
1. foræ¯ä¸€ä¸ªepoch
2. ä»dataloaderé‡Œå–å‡ºä¸€ä¸ªä¸ªbatch
3. æŠŠbatchå–‚ç»™modelï¼ˆå…ˆæŠŠbatchéƒ½ç§»åŠ¨åˆ°å¯¹åº”çš„deviceä¸Šï¼‰
4. æ‹¿å‡ºlossï¼Œè¿›è¡Œåå‘ä¼ æ’­backward
5. åˆ†åˆ«æŠŠoptimizerå’Œscheduleréƒ½æ›´æ–°ä¸€ä¸ªstep

æœ€ååˆ«å¿˜äº†æ¯æ¬¡æ›´æ–°éƒ½è¦æ¸…ç©ºgradï¼Œå³å¯¹optimizerè¿›è¡Œzero_grad()æ“ä½œã€‚


```python
from tqdm import tqdm

for epoch in range(num_epochs):
    for batch in tqdm(train_dataloader):
        # è¦åœ¨GPUä¸Šè®­ç»ƒï¼Œéœ€è¦æŠŠæ•°æ®é›†éƒ½ç§»åŠ¨åˆ°GPUä¸Šï¼š
        batch = {k:v.to(device) for k,v in batch.items()}
        loss = model(**batch).loss
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
```

```shell
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 459/459 [01:54<00:00,  4.01it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 459/459 [01:55<00:00,  3.98it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 459/459 [01:55<00:00,  3.96it/s]
```

## 4. Evaluation

è¿™é‡Œè·Ÿtrain loopè¿˜æ˜¯æŒºç±»ä¼¼çš„ï¼Œä¸€äº›ç»†èŠ‚è§æ³¨é‡Šå³å¯ï¼š


```python
from datasets import load_metric

metric= load_metric("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():  # evaluationçš„æ—¶å€™ä¸éœ€è¦ç®—æ¢¯åº¦
        outputs = model(**batch)
    
    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    # ç”±äºdataloaderæ˜¯æ¯æ¬¡è¾“å‡ºä¸€ä¸ªbatchï¼Œå› æ­¤æˆ‘ä»¬è¦ç­‰ç€æŠŠæ‰€æœ‰batchéƒ½æ·»åŠ è¿›æ¥ï¼Œå†è¿›è¡Œè®¡ç®—
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```


```shell
>>>
{'accuracy': 0.8651960784313726, 'f1': 0.9050086355785838}
```

---

è‡³æ­¤ï¼ŒHuggingface Transformeråˆçº§æ•™ç¨‹å°±å®Œç»“æ’’èŠ±äº†ï¼

<center>ğŸŒ¸ğŸŒ¼ãƒ½(Â°â–½Â°)ãƒğŸŒ¸ğŸŒº</center>

æ›´é«˜çº§çš„æ•™ç¨‹ï¼ŒHuggingfaceä¹Ÿè¿˜æ²¡å‡ºğŸ˜‚ï¼Œæ‰€ä»¥å’±ä»¬æ•¬è¯·æœŸå¾…å§ï¼ä¸è¿‡ï¼Œå­¦å®Œäº†è¿™ä¸ªåˆçº§æ•™ç¨‹ï¼Œæˆ‘ä»¬åŸºæœ¬æ˜¯ä¹Ÿå¯ä»¥å¿«ä¹åœ°æ“ä½œå„ç§å„æ ·Transformer-basedæ¨¡å‹è‡ªç”±ç©è€å•¦ï¼